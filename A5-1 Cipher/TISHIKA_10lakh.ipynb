{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06a47f6c-9e19-4156-86f6-0bb8368aa7fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MY CODE (NOT REQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cff1f4c-bb34-422a-b65c-7215a6a29939",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## NOT REQ (Initial key is in form of bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611e4db2-417c-4af9-9eb7-45e5796fefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43562a45-f864-4207-9f7d-42b96a9d7c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from CSV\n",
    "data = pd.read_csv('keystream_records_4lakh.csv')\n",
    "\n",
    "# Separate the input features (8-byte key) and the targets (10 bytes of keystream)\n",
    "X = data.iloc[:, :8].values  # First 8 columns are the 8-byte key\n",
    "y = data.iloc[:, 8:18].values  # Next 10 columns are the 10 bytes of the keystream\n",
    "\n",
    "# Convert the target to categorical (byte values range from 0 to 255)\n",
    "y = y.astype(np.uint8)\n",
    "\n",
    "# Split the data into training, validation, and testing sets (70-10-20 split)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.67, random_state=42)  # 0.67 * 0.3 â‰ˆ 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eb5c7a8-c591-4a4f-884f-4cd9baef6812",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A51KeystreamPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(A51KeystreamPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        self.fc4 = nn.Linear(10, 10)\n",
    "        self.fc5 = nn.Linear(10, 10)\n",
    "        self.fc6 = nn.Linear(10, 10)\n",
    "        self.fc7 = nn.Linear(10, 10)\n",
    "        self.output = nn.Linear(10, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = torch.softmax(self.output(x), dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f4efb8f-f0a9-4b0a-a3a3-872ba1dbc705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Training for byte 0...\n",
      "Epoch 1/5, Loss: 5.5452, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0040\n",
      "Epoch 2/5, Loss: 5.5452, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0042\n",
      "Epoch 3/5, Loss: 5.5451, Accuracy: 0.0039, Val Loss: 5.5452, Val Accuracy: 0.0041\n",
      "Epoch 4/5, Loss: 5.5451, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0040\n",
      "Epoch 5/5, Loss: 5.5451, Accuracy: 0.0042, Val Loss: 5.5452, Val Accuracy: 0.0040\n",
      "Accuracy for byte 0: 0.38%\n",
      "\n",
      "Training for byte 1...\n",
      "Epoch 1/5, Loss: 5.5452, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0038\n",
      "Epoch 2/5, Loss: 5.5452, Accuracy: 0.0041, Val Loss: 5.5452, Val Accuracy: 0.0039\n",
      "Epoch 3/5, Loss: 5.5451, Accuracy: 0.0041, Val Loss: 5.5452, Val Accuracy: 0.0039\n",
      "Epoch 4/5, Loss: 5.5451, Accuracy: 0.0041, Val Loss: 5.5452, Val Accuracy: 0.0039\n",
      "Epoch 5/5, Loss: 5.5451, Accuracy: 0.0043, Val Loss: 5.5452, Val Accuracy: 0.0045\n",
      "Accuracy for byte 1: 0.39%\n",
      "\n",
      "Training for byte 2...\n",
      "Epoch 1/5, Loss: 5.5452, Accuracy: 0.0039, Val Loss: 5.5452, Val Accuracy: 0.0038\n",
      "Epoch 2/5, Loss: 5.5452, Accuracy: 0.0039, Val Loss: 5.5452, Val Accuracy: 0.0038\n",
      "Epoch 3/5, Loss: 5.5451, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0043\n",
      "Epoch 4/5, Loss: 5.5451, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0044\n",
      "Epoch 5/5, Loss: 5.5451, Accuracy: 0.0042, Val Loss: 5.5452, Val Accuracy: 0.0045\n",
      "Accuracy for byte 2: 0.34%\n",
      "\n",
      "Training for byte 3...\n",
      "Epoch 1/5, Loss: 5.5452, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0042\n",
      "Epoch 2/5, Loss: 5.5452, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0044\n",
      "Epoch 3/5, Loss: 5.5451, Accuracy: 0.0041, Val Loss: 5.5452, Val Accuracy: 0.0039\n",
      "Epoch 4/5, Loss: 5.5451, Accuracy: 0.0043, Val Loss: 5.5452, Val Accuracy: 0.0039\n",
      "Epoch 5/5, Loss: 5.5451, Accuracy: 0.0040, Val Loss: 5.5453, Val Accuracy: 0.0038\n",
      "Accuracy for byte 3: 0.40%\n",
      "\n",
      "Training for byte 4...\n",
      "Epoch 1/5, Loss: 5.5452, Accuracy: 0.0036, Val Loss: 5.5452, Val Accuracy: 0.0041\n",
      "Epoch 2/5, Loss: 5.5452, Accuracy: 0.0041, Val Loss: 5.5452, Val Accuracy: 0.0033\n",
      "Epoch 3/5, Loss: 5.5451, Accuracy: 0.0042, Val Loss: 5.5452, Val Accuracy: 0.0040\n",
      "Epoch 4/5, Loss: 5.5451, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0037\n",
      "Epoch 5/5, Loss: 5.5451, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0033\n",
      "Accuracy for byte 4: 0.38%\n",
      "\n",
      "Training for byte 5...\n",
      "Epoch 1/5, Loss: 5.5452, Accuracy: 0.0039, Val Loss: 5.5452, Val Accuracy: 0.0039\n",
      "Epoch 2/5, Loss: 5.5452, Accuracy: 0.0041, Val Loss: 5.5452, Val Accuracy: 0.0045\n",
      "Epoch 3/5, Loss: 5.5451, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0045\n",
      "Epoch 4/5, Loss: 5.5451, Accuracy: 0.0041, Val Loss: 5.5451, Val Accuracy: 0.0048\n",
      "Epoch 5/5, Loss: 5.5451, Accuracy: 0.0043, Val Loss: 5.5452, Val Accuracy: 0.0039\n",
      "Accuracy for byte 5: 0.35%\n",
      "\n",
      "Training for byte 6...\n",
      "Epoch 1/5, Loss: 5.5452, Accuracy: 0.0041, Val Loss: 5.5452, Val Accuracy: 0.0038\n",
      "Epoch 2/5, Loss: 5.5451, Accuracy: 0.0041, Val Loss: 5.5452, Val Accuracy: 0.0036\n",
      "Epoch 3/5, Loss: 5.5451, Accuracy: 0.0041, Val Loss: 5.5452, Val Accuracy: 0.0035\n",
      "Epoch 4/5, Loss: 5.5451, Accuracy: 0.0042, Val Loss: 5.5452, Val Accuracy: 0.0035\n",
      "Epoch 5/5, Loss: 5.5451, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0032\n",
      "Accuracy for byte 6: 0.37%\n",
      "\n",
      "Training for byte 7...\n",
      "Epoch 1/5, Loss: 5.5452, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0046\n",
      "Epoch 2/5, Loss: 5.5451, Accuracy: 0.0042, Val Loss: 5.5452, Val Accuracy: 0.0041\n",
      "Epoch 3/5, Loss: 5.5451, Accuracy: 0.0042, Val Loss: 5.5452, Val Accuracy: 0.0041\n",
      "Epoch 4/5, Loss: 5.5451, Accuracy: 0.0042, Val Loss: 5.5452, Val Accuracy: 0.0043\n",
      "Epoch 5/5, Loss: 5.5451, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0041\n",
      "Accuracy for byte 7: 0.38%\n",
      "\n",
      "Training for byte 8...\n",
      "Epoch 1/5, Loss: 5.5452, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0038\n",
      "Epoch 2/5, Loss: 5.5452, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0040\n",
      "Epoch 3/5, Loss: 5.5451, Accuracy: 0.0039, Val Loss: 5.5452, Val Accuracy: 0.0040\n",
      "Epoch 4/5, Loss: 5.5451, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0046\n",
      "Epoch 5/5, Loss: 5.5451, Accuracy: 0.0039, Val Loss: 5.5452, Val Accuracy: 0.0040\n",
      "Accuracy for byte 8: 0.44%\n",
      "\n",
      "Training for byte 9...\n",
      "Epoch 1/5, Loss: 5.5452, Accuracy: 0.0038, Val Loss: 5.5452, Val Accuracy: 0.0039\n",
      "Epoch 2/5, Loss: 5.5452, Accuracy: 0.0041, Val Loss: 5.5452, Val Accuracy: 0.0035\n",
      "Epoch 3/5, Loss: 5.5451, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0039\n",
      "Epoch 4/5, Loss: 5.5451, Accuracy: 0.0041, Val Loss: 5.5452, Val Accuracy: 0.0040\n",
      "Epoch 5/5, Loss: 5.5451, Accuracy: 0.0040, Val Loss: 5.5452, Val Accuracy: 0.0039\n",
      "Accuracy for byte 9: 0.42%\n",
      "\n",
      "Accuracies for all 10 bytes:\n",
      "Byte 0: 0.38%\n",
      "Byte 1: 0.39%\n",
      "Byte 2: 0.34%\n",
      "Byte 3: 0.40%\n",
      "Byte 4: 0.38%\n",
      "Byte 5: 0.35%\n",
      "Byte 6: 0.37%\n",
      "Byte 7: 0.38%\n",
      "Byte 8: 0.44%\n",
      "Byte 9: 0.42%\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Function to train and evaluate a model for a specific byte of the keystream\n",
    "def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, byte_index):\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = A51KeystreamPredictor().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()  # Cross-Entropy Loss for classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Convert data to PyTorch tensors and move to GPU if available\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train[:, byte_index], dtype=torch.long).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val[:, byte_index], dtype=torch.long).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test[:, byte_index], dtype=torch.long).to(device)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 5\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training the model\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct_val / total_val\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())\n",
    "        return test_accuracy\n",
    "\n",
    "# Train and evaluate the model for each byte and print the accuracy\n",
    "accuracies = []\n",
    "for byte_index in range(10):\n",
    "    print(f'\\nTraining for byte {byte_index}...')\n",
    "    accuracy = train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, byte_index)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Accuracy for byte {byte_index}: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print all accuracies\n",
    "print(\"\\nAccuracies for all 10 bytes:\")\n",
    "for byte_index, accuracy in enumerate(accuracies):\n",
    "    print(f'Byte {byte_index}: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe5735-643a-40db-af6e-d98349a70810",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## VERY FIRST CODE WITHOUT OVERFIT CHK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebec5725-f060-4c74-a3de-108afc3eaf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3cad8fa-8dd0-4323-abca-ff5f9d56ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from CSV\n",
    "data = pd.read_csv('keystream_records.csv')\n",
    "\n",
    "# Separate the input features (64-bit key) and the targets (10 bytes of keystream)\n",
    "X = data.iloc[:, :64].values  # First 64 columns are the 64-bit key\n",
    "y = data.iloc[:, 64:74].values   # Next 10 columns are the 10 bytes of the keystream\n",
    "\n",
    "# Convert the target to categorical (byte values range from 0 to 255)\n",
    "y = y.astype(np.uint8)\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c60811f9-2623-4063-aed9-c915613680db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeystreamClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeystreamClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ccf75d0-609f-459e-a5be-c619cb48f166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Training for byte 0...\n",
      "Epoch 1/20, Loss: 5.546666142654419\n",
      "Epoch 2/20, Loss: 5.543869366455078\n",
      "Epoch 3/20, Loss: 5.536435187530517\n",
      "Epoch 4/20, Loss: 5.514980736160278\n",
      "Epoch 5/20, Loss: 5.475875542449951\n",
      "Epoch 6/20, Loss: 5.421879569625855\n",
      "Epoch 7/20, Loss: 5.360091432571411\n",
      "Epoch 8/20, Loss: 5.294062951278686\n",
      "Epoch 9/20, Loss: 5.228262239456177\n",
      "Epoch 10/20, Loss: 5.1649985584259035\n",
      "Epoch 11/20, Loss: 5.107959839630127\n",
      "Epoch 12/20, Loss: 5.055957280731201\n",
      "Epoch 13/20, Loss: 5.011220754241943\n",
      "Epoch 14/20, Loss: 4.97015382232666\n",
      "Epoch 15/20, Loss: 4.932823988723755\n",
      "Epoch 16/20, Loss: 4.899966220855713\n",
      "Epoch 17/20, Loss: 4.8698786418914795\n",
      "Epoch 18/20, Loss: 4.842659124374389\n",
      "Epoch 19/20, Loss: 4.818942027664185\n",
      "Epoch 20/20, Loss: 4.7959378032684326\n",
      "Accuracy for byte 0: 0.38%\n",
      "\n",
      "Training for byte 1...\n",
      "Epoch 1/20, Loss: 5.546703688430786\n",
      "Epoch 2/20, Loss: 5.544201615905762\n",
      "Epoch 3/20, Loss: 5.538968728637696\n",
      "Epoch 4/20, Loss: 5.521449309921264\n",
      "Epoch 5/20, Loss: 5.487367841339111\n",
      "Epoch 6/20, Loss: 5.438065752792358\n",
      "Epoch 7/20, Loss: 5.379869708251953\n",
      "Epoch 8/20, Loss: 5.316718106842041\n",
      "Epoch 9/20, Loss: 5.251177769088745\n",
      "Epoch 10/20, Loss: 5.187513980484009\n",
      "Epoch 11/20, Loss: 5.127446676635742\n",
      "Epoch 12/20, Loss: 5.07262481842041\n",
      "Epoch 13/20, Loss: 5.022763321685791\n",
      "Epoch 14/20, Loss: 4.979534225845337\n",
      "Epoch 15/20, Loss: 4.940259523010254\n",
      "Epoch 16/20, Loss: 4.904552332687378\n",
      "Epoch 17/20, Loss: 4.872667440795898\n",
      "Epoch 18/20, Loss: 4.843861302566529\n",
      "Epoch 19/20, Loss: 4.819615003204346\n",
      "Epoch 20/20, Loss: 4.796012684631347\n",
      "Accuracy for byte 1: 0.30%\n",
      "\n",
      "Training for byte 2...\n",
      "Epoch 1/20, Loss: 5.546719675827027\n",
      "Epoch 2/20, Loss: 5.544298569107056\n",
      "Epoch 3/20, Loss: 5.538406896972656\n",
      "Epoch 4/20, Loss: 5.518213036727905\n",
      "Epoch 5/20, Loss: 5.481605573272705\n",
      "Epoch 6/20, Loss: 5.431317367553711\n",
      "Epoch 7/20, Loss: 5.370894047164917\n",
      "Epoch 8/20, Loss: 5.307221157455444\n",
      "Epoch 9/20, Loss: 5.242267392730713\n",
      "Epoch 10/20, Loss: 5.181059498596191\n",
      "Epoch 11/20, Loss: 5.122439246368408\n",
      "Epoch 12/20, Loss: 5.071933741760254\n",
      "Epoch 13/20, Loss: 5.024856885910034\n",
      "Epoch 14/20, Loss: 4.982828905105591\n",
      "Epoch 15/20, Loss: 4.94653998260498\n",
      "Epoch 16/20, Loss: 4.912010438156128\n",
      "Epoch 17/20, Loss: 4.8829180603027345\n",
      "Epoch 18/20, Loss: 4.8556554306030275\n",
      "Epoch 19/20, Loss: 4.831234031295776\n",
      "Epoch 20/20, Loss: 4.80861344909668\n",
      "Accuracy for byte 2: 0.41%\n",
      "\n",
      "Training for byte 3...\n",
      "Epoch 1/20, Loss: 5.5469024974823\n",
      "Epoch 2/20, Loss: 5.544080809783935\n",
      "Epoch 3/20, Loss: 5.5378318122863766\n",
      "Epoch 4/20, Loss: 5.518781153488159\n",
      "Epoch 5/20, Loss: 5.481239572143554\n",
      "Epoch 6/20, Loss: 5.429830816650391\n",
      "Epoch 7/20, Loss: 5.369339078140259\n",
      "Epoch 8/20, Loss: 5.304318910598755\n",
      "Epoch 9/20, Loss: 5.239870299911499\n",
      "Epoch 10/20, Loss: 5.177306657791138\n",
      "Epoch 11/20, Loss: 5.119074362945557\n",
      "Epoch 12/20, Loss: 5.06793376121521\n",
      "Epoch 13/20, Loss: 5.0216910209655765\n",
      "Epoch 14/20, Loss: 4.978180339050293\n",
      "Epoch 15/20, Loss: 4.942601362228394\n",
      "Epoch 16/20, Loss: 4.907836696243286\n",
      "Epoch 17/20, Loss: 4.878446979141235\n",
      "Epoch 18/20, Loss: 4.849551305770874\n",
      "Epoch 19/20, Loss: 4.825983857727051\n",
      "Epoch 20/20, Loss: 4.801054649734497\n",
      "Accuracy for byte 3: 0.38%\n",
      "\n",
      "Training for byte 4...\n",
      "Epoch 1/20, Loss: 5.546580919265747\n",
      "Epoch 2/20, Loss: 5.543683455657959\n",
      "Epoch 3/20, Loss: 5.536799449157715\n",
      "Epoch 4/20, Loss: 5.515573332595825\n",
      "Epoch 5/20, Loss: 5.475734646606445\n",
      "Epoch 6/20, Loss: 5.421866828918457\n",
      "Epoch 7/20, Loss: 5.360717735671997\n",
      "Epoch 8/20, Loss: 5.295464374923706\n",
      "Epoch 9/20, Loss: 5.229827286148072\n",
      "Epoch 10/20, Loss: 5.168176361465454\n",
      "Epoch 11/20, Loss: 5.112648820495606\n",
      "Epoch 12/20, Loss: 5.060107318878174\n",
      "Epoch 13/20, Loss: 5.015774414443969\n",
      "Epoch 14/20, Loss: 4.9756516723632815\n",
      "Epoch 15/20, Loss: 4.939250369644165\n",
      "Epoch 16/20, Loss: 4.90709606590271\n",
      "Epoch 17/20, Loss: 4.878087666320801\n",
      "Epoch 18/20, Loss: 4.8506402545928955\n",
      "Epoch 19/20, Loss: 4.825562126922607\n",
      "Epoch 20/20, Loss: 4.80409987449646\n",
      "Accuracy for byte 4: 0.38%\n",
      "\n",
      "Training for byte 5...\n",
      "Epoch 1/20, Loss: 5.546594865417481\n",
      "Epoch 2/20, Loss: 5.542956926727295\n",
      "Epoch 3/20, Loss: 5.53412430267334\n",
      "Epoch 4/20, Loss: 5.510594972610473\n",
      "Epoch 5/20, Loss: 5.469144262695313\n",
      "Epoch 6/20, Loss: 5.413448770523071\n",
      "Epoch 7/20, Loss: 5.349714145660401\n",
      "Epoch 8/20, Loss: 5.283023321151734\n",
      "Epoch 9/20, Loss: 5.2159602714538575\n",
      "Epoch 10/20, Loss: 5.152851619720459\n",
      "Epoch 11/20, Loss: 5.094099144744873\n",
      "Epoch 12/20, Loss: 5.042502331924439\n",
      "Epoch 13/20, Loss: 4.994466118621826\n",
      "Epoch 14/20, Loss: 4.952545369720459\n",
      "Epoch 15/20, Loss: 4.914738486099243\n",
      "Epoch 16/20, Loss: 4.880647922897339\n",
      "Epoch 17/20, Loss: 4.850312363052368\n",
      "Epoch 18/20, Loss: 4.8249683235168455\n",
      "Epoch 19/20, Loss: 4.798245188903809\n",
      "Epoch 20/20, Loss: 4.776330035400391\n",
      "Accuracy for byte 5: 0.40%\n",
      "\n",
      "Training for byte 6...\n",
      "Epoch 1/20, Loss: 5.546620444107056\n",
      "Epoch 2/20, Loss: 5.543467866516114\n",
      "Epoch 3/20, Loss: 5.53447398223877\n",
      "Epoch 4/20, Loss: 5.509097919464112\n",
      "Epoch 5/20, Loss: 5.466259848403931\n",
      "Epoch 6/20, Loss: 5.411048781967163\n",
      "Epoch 7/20, Loss: 5.3484844219207766\n",
      "Epoch 8/20, Loss: 5.2807850173950195\n",
      "Epoch 9/20, Loss: 5.215074866104126\n",
      "Epoch 10/20, Loss: 5.151397466659546\n",
      "Epoch 11/20, Loss: 5.09162339515686\n",
      "Epoch 12/20, Loss: 5.038730820083618\n",
      "Epoch 13/20, Loss: 4.991688054656983\n",
      "Epoch 14/20, Loss: 4.948384571456909\n",
      "Epoch 15/20, Loss: 4.911461209869385\n",
      "Epoch 16/20, Loss: 4.877062999343872\n",
      "Epoch 17/20, Loss: 4.846354954910279\n",
      "Epoch 18/20, Loss: 4.820330333709717\n",
      "Epoch 19/20, Loss: 4.79463812675476\n",
      "Epoch 20/20, Loss: 4.773139030075074\n",
      "Accuracy for byte 6: 0.34%\n",
      "\n",
      "Training for byte 7...\n",
      "Epoch 1/20, Loss: 5.546889213562012\n",
      "Epoch 2/20, Loss: 5.544425220489502\n",
      "Epoch 3/20, Loss: 5.537199129104614\n",
      "Epoch 4/20, Loss: 5.51506788482666\n",
      "Epoch 5/20, Loss: 5.475209912109375\n",
      "Epoch 6/20, Loss: 5.422406151199341\n",
      "Epoch 7/20, Loss: 5.360679594039917\n",
      "Epoch 8/20, Loss: 5.297154810333252\n",
      "Epoch 9/20, Loss: 5.232381880187988\n",
      "Epoch 10/20, Loss: 5.170763977050782\n",
      "Epoch 11/20, Loss: 5.114712339782715\n",
      "Epoch 12/20, Loss: 5.06454560585022\n",
      "Epoch 13/20, Loss: 5.018368899536132\n",
      "Epoch 14/20, Loss: 4.979004638290405\n",
      "Epoch 15/20, Loss: 4.9428196605682375\n",
      "Epoch 16/20, Loss: 4.911411835479736\n",
      "Epoch 17/20, Loss: 4.881601480865479\n",
      "Epoch 18/20, Loss: 4.855356132888794\n",
      "Epoch 19/20, Loss: 4.831647621154785\n",
      "Epoch 20/20, Loss: 4.810056361007691\n",
      "Accuracy for byte 7: 0.41%\n",
      "\n",
      "Training for byte 8...\n",
      "Epoch 1/20, Loss: 5.546573136138916\n",
      "Epoch 2/20, Loss: 5.54397064743042\n",
      "Epoch 3/20, Loss: 5.537264490509033\n",
      "Epoch 4/20, Loss: 5.515614849090576\n",
      "Epoch 5/20, Loss: 5.476511541748047\n",
      "Epoch 6/20, Loss: 5.422927315139771\n",
      "Epoch 7/20, Loss: 5.3601077350616455\n",
      "Epoch 8/20, Loss: 5.293172423934936\n",
      "Epoch 9/20, Loss: 5.226473591995239\n",
      "Epoch 10/20, Loss: 5.163923357772827\n",
      "Epoch 11/20, Loss: 5.105490490722656\n",
      "Epoch 12/20, Loss: 5.0523358295440675\n",
      "Epoch 13/20, Loss: 5.0062193634033205\n",
      "Epoch 14/20, Loss: 4.965471600723267\n",
      "Epoch 15/20, Loss: 4.928386354827881\n",
      "Epoch 16/20, Loss: 4.893837665176392\n",
      "Epoch 17/20, Loss: 4.8643730350494385\n",
      "Epoch 18/20, Loss: 4.836041650009156\n",
      "Epoch 19/20, Loss: 4.812500802230835\n",
      "Epoch 20/20, Loss: 4.7884316230773925\n",
      "Accuracy for byte 8: 0.40%\n",
      "\n",
      "Training for byte 9...\n",
      "Epoch 1/20, Loss: 5.547037814331055\n",
      "Epoch 2/20, Loss: 5.5437797237396245\n",
      "Epoch 3/20, Loss: 5.536733697891235\n",
      "Epoch 4/20, Loss: 5.516398110198975\n",
      "Epoch 5/20, Loss: 5.47758454284668\n",
      "Epoch 6/20, Loss: 5.42440944404602\n",
      "Epoch 7/20, Loss: 5.361722746658325\n",
      "Epoch 8/20, Loss: 5.296585614776611\n",
      "Epoch 9/20, Loss: 5.229997632980346\n",
      "Epoch 10/20, Loss: 5.167793424987793\n",
      "Epoch 11/20, Loss: 5.111288543319702\n",
      "Epoch 12/20, Loss: 5.0595823085784914\n",
      "Epoch 13/20, Loss: 5.014989984512329\n",
      "Epoch 14/20, Loss: 4.974260537338257\n",
      "Epoch 15/20, Loss: 4.936783822631836\n",
      "Epoch 16/20, Loss: 4.903540829086304\n",
      "Epoch 17/20, Loss: 4.873975218200684\n",
      "Epoch 18/20, Loss: 4.847563450241089\n",
      "Epoch 19/20, Loss: 4.823003567123413\n",
      "Epoch 20/20, Loss: 4.801027988815307\n",
      "Accuracy for byte 9: 0.41%\n",
      "\n",
      "Accuracies for all 10 bytes:\n",
      "Byte 0: 0.38%\n",
      "Byte 1: 0.30%\n",
      "Byte 2: 0.41%\n",
      "Byte 3: 0.38%\n",
      "Byte 4: 0.38%\n",
      "Byte 5: 0.40%\n",
      "Byte 6: 0.34%\n",
      "Byte 7: 0.41%\n",
      "Byte 8: 0.40%\n",
      "Byte 9: 0.41%\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Function to train and evaluate a model for a specific byte of the keystream\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, byte_index):\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = KeystreamClassifier().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Convert data to PyTorch tensors and move to GPU if available\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train[:, byte_index], dtype=torch.long).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test[:, byte_index], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # Training the model\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())\n",
    "        return accuracy\n",
    "\n",
    "# Train and evaluate the model for each byte and print the accuracy\n",
    "accuracies = []\n",
    "for byte_index in range(10):\n",
    "    print(f'\\nTraining for byte {byte_index}...')\n",
    "    accuracy = train_and_evaluate(X_train, y_train, X_test, y_test, byte_index)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Accuracy for byte {byte_index}: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print all accuracies\n",
    "print(\"\\nAccuracies for all 10 bytes:\")\n",
    "for byte_index, accuracy in enumerate(accuracies):\n",
    "    print(f'Byte {byte_index}: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed89ef4-d20a-4698-a9a2-38027faa24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from CSV\n",
    "data = pd.read_csv('keystream_records_10lakh.csv')\n",
    "\n",
    "# Separate the input features (64-bit key) and the targets (10 bytes of keystream)\n",
    "X = data.iloc[:, :64].values  # First 64 columns are the 64-bit key\n",
    "y = data.iloc[:, 64:74].values   # Next 10 columns are the 10 bytes of the keystream\n",
    "\n",
    "# Convert the target to categorical (byte values range from 0 to 255)\n",
    "y = y.astype(np.uint8)\n",
    "\n",
    "# Split the data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e90a03-9a09-4894-b545-952f76ac29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Function to train and evaluate a model for a specific byte of the keystream\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, byte_index):\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = KeystreamClassifier().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Convert data to PyTorch tensors and move to GPU if available\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train[:, byte_index], dtype=torch.long).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test[:, byte_index], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # Training the model\n",
    "    num_epochs = 50\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        accuracy = accuracy_score(y_test_tensor.cpu(), predicted.cpu())\n",
    "        return accuracy\n",
    "\n",
    "# Train and evaluate the model for each byte and print the accuracy\n",
    "accuracies = []\n",
    "for byte_index in range(10):\n",
    "    print(f'\\nTraining for byte {byte_index}...')\n",
    "    accuracy = train_and_evaluate(X_train, y_train, X_test, y_test, byte_index)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Accuracy for byte {byte_index}: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Print all accuracies\n",
    "print(\"\\nAccuracies for all 10 bytes:\")\n",
    "for byte_index, accuracy in enumerate(accuracies):\n",
    "    print(f'Byte {byte_index}: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c7780f-2657-4798-bb26-e64b8adc7d2a",
   "metadata": {},
   "source": [
    "# TISHIKA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33693e3e-4160-44c5-882e-740867a27d68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## NOT WORKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93f5d418-acf1-46b4-ab79-64961560c3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for byte 0...\n",
      "33750/33750 [==============================] - 256s 7ms/step - loss: 5.5776 - accuracy: 0.0039 - val_loss: 5.5535 - val_accuracy: 0.0041\n",
      "Accuracy for byte 0: 0.39%\n",
      "\n",
      "Training for byte 1...\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m byte_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# 10 bytes\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining for byte \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbyte_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbyte_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     accuracies\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy for byte \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbyte_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(X_train, y_train, X_test, y_test, byte_index)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Transfer data to GPU\u001b[39;00m\n\u001b[0;32m     14\u001b[0m X_train_gpu \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(X_train_reshaped)\n\u001b[1;32m---> 15\u001b[0m y_train_gpu \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train_byte\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m X_test_gpu \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(X_test_reshaped)\n\u001b[0;32m     17\u001b[0m y_test_gpu \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_test_byte)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensor\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "# Function to train and evaluate a model for a specific byte of the keystream\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, byte_index):\n",
    "    # Initialize the model\n",
    "    model = build_model()\n",
    "\n",
    "    # Convert the labels to categorical\n",
    "    y_train_byte = to_categorical(y_train[:, byte_index], num_classes=256)  # 256 possible byte values\n",
    "    y_test_byte = to_categorical(y_test[:, byte_index], num_classes=256)\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "    # Transfer data to GPU\n",
    "    X_train_gpu = tf.convert_to_tensor(X_train_reshaped)\n",
    "    y_train_gpu = tf.convert_to_tensor(y_train_byte)\n",
    "    X_test_gpu = tf.convert_to_tensor(X_test_reshaped)\n",
    "    y_test_gpu = tf.convert_to_tensor(y_test_byte)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_gpu, y_train_gpu, epochs=1, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(X_test_gpu, y_test_gpu, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# Train and evaluate the model for each byte and print the accuracy\n",
    "accuracies = []\n",
    "for byte_index in range(10):  # 10 bytes\n",
    "    print(f'\\nTraining for byte {byte_index}...')\n",
    "    accuracy = train_and_evaluate(X_train, y_train, X_test, y_test, byte_index)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Accuracy for byte {byte_index}: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdca02b-3004-4a6e-88dc-fb7c5bbfa7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all accuracies\n",
    "print(\"Accuracies for all 10 bytes:\")\n",
    "for byte_index, accuracy in enumerate(accuracies):\n",
    "    print(f'Byte {byte_index}: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6cd8e-fef2-47d6-b380-54b93cbdcdd2",
   "metadata": {},
   "source": [
    "## WORKING 10 LAKH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666671d4-2200-4ac0-a218-b9853b70d120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the data from CSV\n",
    "data = pd.read_csv('keystream_records_10lakh.csv')\n",
    "\n",
    "# Separate the input features (64-bit key) and the targets (10 bytes of keystream)\n",
    "X = data.iloc[:, :64].values  # First 64 columns are the 64-bit key\n",
    "y = data.iloc[:, 64:74].values  # Next 10 columns are the 10 bytes of the keystream\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape X to be (num_samples, 64, 1) for Conv1D\n",
    "X_train_reshaped = X_train.reshape(-1, 64, 1)\n",
    "X_test_reshaped = X_test.reshape(-1, 64, 1)\n",
    "\n",
    "# Function to build the model\n",
    "def build_model():\n",
    "    input_layer = Input(shape=(64, 1))\n",
    "    x = Conv1D(32, 4, activation='relu', padding='same', strides=2)(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(64, 4, activation='relu', padding='same', strides=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(128, 4, activation='relu', padding='same', strides=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(256, 4, activation='relu', padding='same', strides=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    output_layer = Dense(256, activation='softmax')(x)  # 256 units for each byte\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate a model for a specific byte of the keystream\n",
    "def train_and_evaluate_byte_0(X_train, y_train, X_test, y_test):\n",
    "    byte_index = 0\n",
    "    model = build_model()\n",
    "    y_train_byte = to_categorical(y_train[:, byte_index], num_classes=256)\n",
    "    y_test_byte = to_categorical(y_test[:, byte_index], num_classes=256)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    X_train_gpu = tf.convert_to_tensor(X_train_reshaped)\n",
    "    y_train_gpu = tf.convert_to_tensor(y_train_byte)\n",
    "    X_test_gpu = tf.convert_to_tensor(X_test_reshaped)\n",
    "    y_test_gpu = tf.convert_to_tensor(y_test_byte)\n",
    "    model.fit(X_train_gpu, y_train_gpu, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "    loss, accuracy = model.evaluate(X_test_gpu, y_test_gpu, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def train_and_evaluate_byte_1(X_train, y_train, X_test, y_test):\n",
    "    byte_index = 1\n",
    "    model = build_model()\n",
    "    y_train_byte = to_categorical(y_train[:, byte_index], num_classes=256)\n",
    "    y_test_byte = to_categorical(y_test[:, byte_index], num_classes=256)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    X_train_gpu = tf.convert_to_tensor(X_train_reshaped)\n",
    "    y_train_gpu = tf.convert_to_tensor(y_train_byte)\n",
    "    X_test_gpu = tf.convert_to_tensor(X_test_reshaped)\n",
    "    y_test_gpu = tf.convert_to_tensor(y_test_byte)\n",
    "    model.fit(X_train_gpu, y_train_gpu, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "    loss, accuracy = model.evaluate(X_test_gpu, y_test_gpu, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# Repeat similar function definitions for bytes 2 through 9\n",
    "\n",
    "def train_and_evaluate_byte_2(X_train, y_train, X_test, y_test):\n",
    "    byte_index = 2\n",
    "    model = build_model()\n",
    "    y_train_byte = to_categorical(y_train[:, byte_index], num_classes=256)\n",
    "    y_test_byte = to_categorical(y_test[:, byte_index], num_classes=256)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    X_train_gpu = tf.convert_to_tensor(X_train_reshaped)\n",
    "    y_train_gpu = tf.convert_to_tensor(y_train_byte)\n",
    "    X_test_gpu = tf.convert_to_tensor(X_test_reshaped)\n",
    "    y_test_gpu = tf.convert_to_tensor(y_test_byte)\n",
    "    model.fit(X_train_gpu, y_train_gpu, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "    loss, accuracy = model.evaluate(X_test_gpu, y_test_gpu, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def train_and_evaluate_byte_3(X_train, y_train, X_test, y_test):\n",
    "    byte_index = 3\n",
    "    model = build_model()\n",
    "    y_train_byte = to_categorical(y_train[:, byte_index], num_classes=256)\n",
    "    y_test_byte = to_categorical(y_test[:, byte_index], num_classes=256)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    X_train_gpu = tf.convert_to_tensor(X_train_reshaped)\n",
    "    y_train_gpu = tf.convert_to_tensor(y_train_byte)\n",
    "    X_test_gpu = tf.convert_to_tensor(X_test_reshaped)\n",
    "    y_test_gpu = tf.convert_to_tensor(y_test_byte)\n",
    "    model.fit(X_train_gpu, y_train_gpu, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "    loss, accuracy = model.evaluate(X_test_gpu, y_test_gpu, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def train_and_evaluate_byte_4(X_train, y_train, X_test, y_test):\n",
    "    byte_index = 4\n",
    "    model = build_model()\n",
    "    y_train_byte = to_categorical(y_train[:, byte_index], num_classes=256)\n",
    "    y_test_byte = to_categorical(y_test[:, byte_index], num_classes=256)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    X_train_gpu = tf.convert_to_tensor(X_train_reshaped)\n",
    "    y_train_gpu = tf.convert_to_tensor(y_train_byte)\n",
    "    X_test_gpu = tf.convert_to_tensor(X_test_reshaped)\n",
    "    y_test_gpu = tf.convert_to_tensor(y_test_byte)\n",
    "    model.fit(X_train_gpu, y_train_gpu, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "    loss, accuracy = model.evaluate(X_test_gpu, y_test_gpu, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def train_and_evaluate_byte_5(X_train, y_train, X_test, y_test):\n",
    "    byte_index = 5\n",
    "    model = build_model()\n",
    "    y_train_byte = to_categorical(y_train[:, byte_index], num_classes=256)\n",
    "    y_test_byte = to_categorical(y_test[:, byte_index], num_classes=256)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    X_train_gpu = tf.convert_to_tensor(X_train_reshaped)\n",
    "    y_train_gpu = tf.convert_to_tensor(y_train_byte)\n",
    "    X_test_gpu = tf.convert_to_tensor(X_test_reshaped)\n",
    "    y_test_gpu = tf.convert_to_tensor(y_test_byte)\n",
    "    model.fit(X_train_gpu, y_train_gpu, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "    loss, accuracy = model.evaluate(X_test_gpu, y_test_gpu, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def train_and_evaluate_byte_6(X_train, y_train, X_test, y_test):\n",
    "    byte_index = 6\n",
    "    model = build_model()\n",
    "    y_train_byte = to_categorical(y_train[:, byte_index], num_classes=256)\n",
    "    y_test_byte = to_categorical(y_test[:, byte_index], num_classes=256)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    X_train_gpu = tf.convert_to_tensor(X_train_reshaped)\n",
    "    y_train_gpu = tf.convert_to_tensor(y_train_byte)\n",
    "    X_test_gpu = tf.convert_to_tensor(X_test_reshaped)\n",
    "    y_test_gpu = tf.convert_to_tensor(y_test_byte)\n",
    "    model.fit(X_train_gpu, y_train_gpu, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "    loss, accuracy = model.evaluate(X_test_gpu, y_test_gpu, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def train_and_evaluate_byte_7(X_train, y_train, X_test, y_test):\n",
    "    byte_index = 7\n",
    "    model = build_model()\n",
    "    y_train_byte = to_categorical(y_train[:, byte_index], num_classes=256)\n",
    "    y_test_byte = to_categorical(y_test[:, byte_index], num_classes=256)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    X_train_gpu = tf.convert_to_tensor(X_train_reshaped)\n",
    "    y_train_gpu = tf.convert_to_tensor(y_train_byte)\n",
    "    X_test_gpu = tf.convert_to_tensor(X_test_reshaped)\n",
    "    y_test_gpu = tf.convert_to_tensor(y_test_byte)\n",
    "    model.fit(X_train_gpu, y_train_gpu, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "    loss, accuracy = model.evaluate(X_test_gpu, y_test_gpu, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def train_and_evaluate_byte_8(X_train, y_train, X_test, y_test):\n",
    "    byte_index = 8\n",
    "    model = build_model()\n",
    "    y_train_byte = to_categorical(y_train[:, byte_index], num_classes=256)\n",
    "    y_test_byte = to_categorical(y_test[:, byte_index], num_classes=256)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    X_train_gpu = tf.convert_to_tensor(X_train_reshaped)\n",
    "    y_train_gpu = tf.convert_to_tensor(y_train_byte)\n",
    "    X_test_gpu = tf.convert_to_tensor(X_test_reshaped)\n",
    "    y_test_gpu = tf.convert_to_tensor(y_test_byte)\n",
    "    model.fit(X_train_gpu, y_train_gpu, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "    loss, accuracy = model.evaluate(X_test_gpu, y_test_gpu, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "def train_and_evaluate_byte_9(X_train, y_train, X_test, y_test):\n",
    "    byte_index = 9\n",
    "    model = build_model()\n",
    "    y_train_byte = to_categorical(y_train[:, byte_index], num_classes=256)\n",
    "    y_test_byte = to_categorical(y_test[:, byte_index], num_classes=256)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    X_train_gpu = tf.convert_to_tensor(X_train_reshaped)\n",
    "    y_train_gpu = tf.convert_to_tensor(y_train_byte)\n",
    "    X_test_gpu = tf.convert_to_tensor(X_test_reshaped)\n",
    "    y_test_gpu = tf.convert_to_tensor(y_test_byte)\n",
    "    model.fit(X_train_gpu, y_train_gpu, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=1)\n",
    "    loss, accuracy = model.evaluate(X_test_gpu, y_test_gpu, verbose=0)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8124aff7-12b6-4a55-ae27-c4a849f16aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for byte 0...\n",
      "Epoch 1/50\n",
      "33750/33750 [==============================] - 245s 7ms/step - loss: 5.5779 - accuracy: 0.0039 - val_loss: 5.5521 - val_accuracy: 0.0042\n",
      "Epoch 2/50\n",
      "33750/33750 [==============================] - 245s 7ms/step - loss: 5.5531 - accuracy: 0.0039 - val_loss: 5.5534 - val_accuracy: 0.0036\n",
      "Epoch 3/50\n",
      "33750/33750 [==============================] - 238s 7ms/step - loss: 5.5541 - accuracy: 0.0038 - val_loss: 5.5548 - val_accuracy: 0.0042\n",
      "Epoch 4/50\n",
      "33750/33750 [==============================] - 243s 7ms/step - loss: 5.5543 - accuracy: 0.0040 - val_loss: 5.5542 - val_accuracy: 0.0039\n",
      "Epoch 5/50\n",
      "33750/33750 [==============================] - 242s 7ms/step - loss: 5.5541 - accuracy: 0.0041 - val_loss: 5.5556 - val_accuracy: 0.0041\n",
      "Epoch 6/50\n",
      "33750/33750 [==============================] - 243s 7ms/step - loss: 5.5535 - accuracy: 0.0042 - val_loss: 5.5543 - val_accuracy: 0.0039\n",
      "Accuracy for byte 0: 0.40%\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model for each byte and print the accuracy\n",
    "#accuracies = []\n",
    "\n",
    "print('\\nTraining for byte 0...')\n",
    "accuracy = train_and_evaluate_byte_0(X_train, y_train, X_test, y_test)\n",
    "#accuracies.append(accuracy)\n",
    "print(f'Accuracy for byte 0: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693e5328-dd38-493f-a8f7-d423cbb71e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for byte 1...\n",
      "Epoch 1/50\n",
      "33750/33750 [==============================] - 226s 7ms/step - loss: 5.5779 - accuracy: 0.0038 - val_loss: 5.5508 - val_accuracy: 0.0040\n",
      "Epoch 2/50\n",
      "33750/33750 [==============================] - 227s 7ms/step - loss: 5.5530 - accuracy: 0.0039 - val_loss: 5.5547 - val_accuracy: 0.0039\n",
      "Epoch 3/50\n",
      "33750/33750 [==============================] - 228s 7ms/step - loss: 5.5541 - accuracy: 0.0038 - val_loss: 5.5550 - val_accuracy: 0.0036\n",
      "Epoch 4/50\n",
      "33750/33750 [==============================] - 225s 7ms/step - loss: 5.5542 - accuracy: 0.0040 - val_loss: 5.5560 - val_accuracy: 0.0041\n",
      "Epoch 5/50\n",
      "33750/33750 [==============================] - 262s 8ms/step - loss: 5.5540 - accuracy: 0.0040 - val_loss: 5.5572 - val_accuracy: 0.0039\n",
      "Epoch 6/50\n",
      "33750/33750 [==============================] - 259s 8ms/step - loss: 5.5533 - accuracy: 0.0043 - val_loss: 5.5588 - val_accuracy: 0.0041\n",
      "Accuracy for byte 1: 0.40%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining for byte 1...')\n",
    "accuracy = train_and_evaluate_byte_1(X_train, y_train, X_test, y_test)\n",
    "#accuracies.append(accuracy)\n",
    "print(f'Accuracy for byte 1: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ada6473-e7c1-4b79-ba4a-4705e58fa041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for byte 2...\n",
      "Epoch 1/50\n",
      "33750/33750 [==============================] - 292s 9ms/step - loss: 5.5775 - accuracy: 0.0040 - val_loss: 5.5529 - val_accuracy: 0.0036\n",
      "Epoch 2/50\n",
      "33750/33750 [==============================] - 309s 9ms/step - loss: 5.5531 - accuracy: 0.0039 - val_loss: 5.5528 - val_accuracy: 0.0037\n",
      "Epoch 3/50\n",
      "33750/33750 [==============================] - 328s 10ms/step - loss: 5.5539 - accuracy: 0.0040 - val_loss: 5.5574 - val_accuracy: 0.0039\n",
      "Epoch 4/50\n",
      "33750/33750 [==============================] - 285s 8ms/step - loss: 5.5542 - accuracy: 0.0040 - val_loss: 5.5564 - val_accuracy: 0.0040\n",
      "Epoch 5/50\n",
      "33750/33750 [==============================] - 687s 20ms/step - loss: 5.5539 - accuracy: 0.0041 - val_loss: 5.5571 - val_accuracy: 0.0038\n",
      "Epoch 6/50\n",
      "33750/33750 [==============================] - 239s 7ms/step - loss: 5.5530 - accuracy: 0.0043 - val_loss: 5.5565 - val_accuracy: 0.0042\n",
      "Epoch 7/50\n",
      "33750/33750 [==============================] - 233s 7ms/step - loss: 5.5519 - accuracy: 0.0045 - val_loss: 5.5582 - val_accuracy: 0.0039\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining for byte 2...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m train_and_evaluate_byte_2(X_train, y_train, X_test, y_test)\n\u001b[1;32m----> 3\u001b[0m \u001b[43maccuracies\u001b[49m\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy for byte 2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "print('\\nTraining for byte 2...')\n",
    "accuracy = train_and_evaluate_byte_2(X_train, y_train, X_test, y_test)\n",
    "#accuracies.append(accuracy)\n",
    "print(f'Accuracy for byte 2: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11754122-587e-4bbb-beb9-2ce567bcaff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for byte 2: 0.40%\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy for byte 2: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b875f88-4bdd-4723-a076-dba37e7a8559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for byte 3...\n",
      "Epoch 1/50\n",
      "33750/33750 [==============================] - 279s 8ms/step - loss: 5.5777 - accuracy: 0.0039 - val_loss: 5.5529 - val_accuracy: 0.0038\n",
      "Epoch 2/50\n",
      "33750/33750 [==============================] - 263s 8ms/step - loss: 5.5530 - accuracy: 0.0039 - val_loss: 5.5551 - val_accuracy: 0.0038\n",
      "Epoch 3/50\n",
      "33750/33750 [==============================] - 272s 8ms/step - loss: 5.5540 - accuracy: 0.0040 - val_loss: 5.5536 - val_accuracy: 0.0041\n",
      "Epoch 4/50\n",
      "33750/33750 [==============================] - 263s 8ms/step - loss: 5.5542 - accuracy: 0.0041 - val_loss: 5.5555 - val_accuracy: 0.0039\n",
      "Epoch 5/50\n",
      "33750/33750 [==============================] - 272s 8ms/step - loss: 5.5537 - accuracy: 0.0041 - val_loss: 5.5577 - val_accuracy: 0.0038\n",
      "Epoch 6/50\n",
      "33750/33750 [==============================] - 265s 8ms/step - loss: 5.5528 - accuracy: 0.0043 - val_loss: 5.5574 - val_accuracy: 0.0040\n",
      "Accuracy for byte 3: 0.38%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining for byte 3...')\n",
    "accuracy = train_and_evaluate_byte_3(X_train, y_train, X_test, y_test)\n",
    "#accuracies.append(accuracy)\n",
    "print(f'Accuracy for byte 3: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3135efcd-3e49-49e7-8ff5-18210fe2b664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for byte 4...\n",
      "Epoch 1/50\n",
      "33750/33750 [==============================] - 301s 7ms/step - loss: 5.5779 - accuracy: 0.0039 - val_loss: 5.5511 - val_accuracy: 0.0040\n",
      "Epoch 2/50\n",
      "33750/33750 [==============================] - 239s 7ms/step - loss: 5.5530 - accuracy: 0.0038 - val_loss: 5.5548 - val_accuracy: 0.0040\n",
      "Epoch 3/50\n",
      "33750/33750 [==============================] - 239s 7ms/step - loss: 5.5540 - accuracy: 0.0040 - val_loss: 5.5550 - val_accuracy: 0.0039\n",
      "Epoch 4/50\n",
      "33750/33750 [==============================] - 238s 7ms/step - loss: 5.5541 - accuracy: 0.0041 - val_loss: 5.5562 - val_accuracy: 0.0040\n",
      "Epoch 5/50\n",
      "33750/33750 [==============================] - 228s 7ms/step - loss: 5.5534 - accuracy: 0.0041 - val_loss: 5.5593 - val_accuracy: 0.0036\n",
      "Epoch 6/50\n",
      "33750/33750 [==============================] - 303s 9ms/step - loss: 5.5523 - accuracy: 0.0044 - val_loss: 5.5590 - val_accuracy: 0.0039\n",
      "Accuracy for byte 4: 0.38%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining for byte 4...')\n",
    "accuracy = train_and_evaluate_byte_4(X_train, y_train, X_test, y_test)\n",
    "#accuracies.append(accuracy)\n",
    "print(f'Accuracy for byte 4: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ac5f04-3eb2-4c67-8cda-7b77bbdbeb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for byte 5...\n",
      "Epoch 1/50\n",
      "33750/33750 [==============================] - 263s 8ms/step - loss: 5.5777 - accuracy: 0.0040 - val_loss: 5.5531 - val_accuracy: 0.0038\n",
      "Epoch 2/50\n",
      "33750/33750 [==============================] - 246s 7ms/step - loss: 5.5530 - accuracy: 0.0039 - val_loss: 5.5517 - val_accuracy: 0.0037\n",
      "Epoch 3/50\n",
      "33750/33750 [==============================] - 238s 7ms/step - loss: 5.5538 - accuracy: 0.0039 - val_loss: 5.5546 - val_accuracy: 0.0040\n",
      "Epoch 4/50\n",
      "33750/33750 [==============================] - 250s 7ms/step - loss: 5.5543 - accuracy: 0.0039 - val_loss: 5.5555 - val_accuracy: 0.0041\n",
      "Epoch 5/50\n",
      "33750/33750 [==============================] - 256s 8ms/step - loss: 5.5540 - accuracy: 0.0042 - val_loss: 5.5564 - val_accuracy: 0.0037\n",
      "Epoch 6/50\n",
      "33750/33750 [==============================] - 264s 8ms/step - loss: 5.5534 - accuracy: 0.0041 - val_loss: 5.5564 - val_accuracy: 0.0039\n",
      "Epoch 7/50\n",
      "33750/33750 [==============================] - 253s 7ms/step - loss: 5.5523 - accuracy: 0.0043 - val_loss: 5.5563 - val_accuracy: 0.0037\n",
      "Accuracy for byte 5: 0.39%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining for byte 5...')\n",
    "accuracy = train_and_evaluate_byte_5(X_train, y_train, X_test, y_test)\n",
    "#accuracies.append(accuracy)\n",
    "print(f'Accuracy for byte 5: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4faa486a-3ec9-4a20-8baa-8ed924006d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for byte 6...\n",
      "Epoch 1/50\n",
      "33750/33750 [==============================] - 244s 7ms/step - loss: 5.5776 - accuracy: 0.0039 - val_loss: 5.5521 - val_accuracy: 0.0037\n",
      "Epoch 2/50\n",
      "33750/33750 [==============================] - 250s 7ms/step - loss: 5.5529 - accuracy: 0.0039 - val_loss: 5.5561 - val_accuracy: 0.0038\n",
      "Epoch 3/50\n",
      "33750/33750 [==============================] - 256s 8ms/step - loss: 5.5541 - accuracy: 0.0039 - val_loss: 5.5557 - val_accuracy: 0.0038\n",
      "Epoch 4/50\n",
      "33750/33750 [==============================] - 248s 7ms/step - loss: 5.5542 - accuracy: 0.0040 - val_loss: 5.5548 - val_accuracy: 0.0043\n",
      "Epoch 5/50\n",
      "33750/33750 [==============================] - 264s 8ms/step - loss: 5.5541 - accuracy: 0.0041 - val_loss: 5.5569 - val_accuracy: 0.0041\n",
      "Epoch 6/50\n",
      "33750/33750 [==============================] - 270s 8ms/step - loss: 5.5534 - accuracy: 0.0043 - val_loss: 5.5570 - val_accuracy: 0.0040\n",
      "Accuracy for byte 6: 0.39%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining for byte 6...')\n",
    "accuracy = train_and_evaluate_byte_6(X_train, y_train, X_test, y_test)\n",
    "#accuracies.append(accuracy)\n",
    "print(f'Accuracy for byte 6: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b76bf48-60f1-4aa4-a2ad-5e434353f564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for byte 7...\n",
      "Epoch 1/50\n",
      "33750/33750 [==============================] - 229s 7ms/step - loss: 5.5780 - accuracy: 0.0038 - val_loss: 5.5528 - val_accuracy: 0.0038\n",
      "Epoch 2/50\n",
      "33750/33750 [==============================] - 237s 7ms/step - loss: 5.5531 - accuracy: 0.0040 - val_loss: 5.5526 - val_accuracy: 0.0042\n",
      "Epoch 3/50\n",
      "33750/33750 [==============================] - 236s 7ms/step - loss: 5.5540 - accuracy: 0.0039 - val_loss: 5.5531 - val_accuracy: 0.0038\n",
      "Epoch 4/50\n",
      "33750/33750 [==============================] - 244s 7ms/step - loss: 5.5542 - accuracy: 0.0039 - val_loss: 5.5557 - val_accuracy: 0.0036\n",
      "Epoch 5/50\n",
      "33750/33750 [==============================] - 332s 10ms/step - loss: 5.5540 - accuracy: 0.0043 - val_loss: 5.5551 - val_accuracy: 0.0041\n",
      "Epoch 6/50\n",
      "33750/33750 [==============================] - 372s 11ms/step - loss: 5.5531 - accuracy: 0.0041 - val_loss: 5.5584 - val_accuracy: 0.0039\n",
      "Epoch 7/50\n",
      "33750/33750 [==============================] - 373s 11ms/step - loss: 5.5513 - accuracy: 0.0045 - val_loss: 5.5592 - val_accuracy: 0.0042\n",
      "Accuracy for byte 7: 0.38%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining for byte 7...')\n",
    "accuracy = train_and_evaluate_byte_7(X_train, y_train, X_test, y_test)\n",
    "#accuracies.append(accuracy)\n",
    "print(f'Accuracy for byte 7: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca585c9-685d-4868-9f1a-925f6347206f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for byte 8...\n",
      "Epoch 1/50\n",
      "33750/33750 [==============================] - 237s 7ms/step - loss: 5.5777 - accuracy: 0.0039 - val_loss: 5.5511 - val_accuracy: 0.0040\n",
      "Epoch 2/50\n",
      "33750/33750 [==============================] - 229s 7ms/step - loss: 5.5530 - accuracy: 0.0039 - val_loss: 5.5551 - val_accuracy: 0.0037\n",
      "Epoch 3/50\n",
      "33750/33750 [==============================] - 234s 7ms/step - loss: 5.5541 - accuracy: 0.0040 - val_loss: 5.5539 - val_accuracy: 0.0036\n",
      "Epoch 4/50\n",
      "33750/33750 [==============================] - 232s 7ms/step - loss: 5.5543 - accuracy: 0.0041 - val_loss: 5.5541 - val_accuracy: 0.0039\n",
      "Epoch 5/50\n",
      "33750/33750 [==============================] - 229s 7ms/step - loss: 5.5540 - accuracy: 0.0042 - val_loss: 5.5581 - val_accuracy: 0.0038\n",
      "Epoch 6/50\n",
      "33750/33750 [==============================] - 223s 7ms/step - loss: 5.5533 - accuracy: 0.0042 - val_loss: 5.5579 - val_accuracy: 0.0039\n",
      "Accuracy for byte 8: 0.40%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining for byte 8...')\n",
    "accuracy = train_and_evaluate_byte_8(X_train, y_train, X_test, y_test)\n",
    "#accuracies.append(accuracy)\n",
    "print(f'Accuracy for byte 8: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5478226-b2b2-47e2-bc64-e3780a3f39ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for byte 9...\n",
      "Epoch 1/50\n",
      "33750/33750 [==============================] - 226s 7ms/step - loss: 5.5779 - accuracy: 0.0039 - val_loss: 5.5525 - val_accuracy: 0.0039\n",
      "Epoch 2/50\n",
      "33750/33750 [==============================] - 218s 6ms/step - loss: 5.5530 - accuracy: 0.0039 - val_loss: 5.5537 - val_accuracy: 0.0038\n",
      "Epoch 3/50\n",
      "33750/33750 [==============================] - 221s 7ms/step - loss: 5.5539 - accuracy: 0.0040 - val_loss: 5.5549 - val_accuracy: 0.0037\n",
      "Epoch 4/50\n",
      "33750/33750 [==============================] - 221s 7ms/step - loss: 5.5542 - accuracy: 0.0039 - val_loss: 5.5546 - val_accuracy: 0.0038\n",
      "Epoch 5/50\n",
      "33750/33750 [==============================] - 221s 7ms/step - loss: 5.5537 - accuracy: 0.0042 - val_loss: 5.5588 - val_accuracy: 0.0035\n",
      "Epoch 6/50\n",
      "33750/33750 [==============================] - 225s 7ms/step - loss: 5.5528 - accuracy: 0.0042 - val_loss: 5.5577 - val_accuracy: 0.0038\n",
      "Accuracy for byte 9: 0.40%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining for byte 9...')\n",
    "accuracy = train_and_evaluate_byte_9(X_train, y_train, X_test, y_test)\n",
    "#accuracies.append(accuracy)\n",
    "print(f'Accuracy for byte 9: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242cb65-6bf1-415b-9f25-a58eb93ef931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b324294-75a0-4b54-982b-4573545195e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a7b92-f0ff-46d7-9463-56940145d1cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c4811-e1a4-4ed8-a09e-5ca1fe430cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a459f39b-eb34-4fc0-9960-deaad71022f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4bbb1-4d97-4729-a655-51f773540561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89735571-d065-4bcc-be93-0750f18388cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a3121-b1bd-4de6-845d-c571516ca641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61721c6e-1b75-4979-9b13-5d314cffe568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
